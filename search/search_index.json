{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About \u00b6 Author: Arman Zarei | Advisor: Tyson Lee Swetnam Research Field: Machine Learning & Computer Vision","title":"Introduction"},{"location":"#about","text":"Author: Arman Zarei | Advisor: Tyson Lee Swetnam Research Field: Machine Learning & Computer Vision","title":"About"},{"location":"articles/RSNet/main/","text":"Problem with other semantic segmentation networks \u00b6 Not modelling required dependencies between cloud points. Key component in RSNet \u00b6 lightweight local dependency module \u00b6 The local dependency module is highly efficient and has the time complexity of the slice pooling/unpooling layer as \\(O(n)\\) w.r.t the number of input points and \\(O(1)\\) w.r.t the local context resolutions. RSNet Components \u00b6 Input feature extraction block \u00b6 input points \\(\\rightarrow\\) features Local dependency module \u00b6 Combination of Slice pooling layer Bidirectional Recurrent Neural Network (RNN) layers Slice unpooling layer local context problem is solved by first projecting unordered points into ordered features and then applying traditional end-to-end learning algorithms The projection is achieved by a novel slice pooling layer Slice pooling layer input: features of unordered points \\(\\rightarrow\\) output: ordered sequence of aggregated features RNN layers Next, RNNs are applied to model dependencies in this sequence. Slice unpooling layer Finally, a slice unpooling layer assigns features in the sequence back to points . Output feature extraction block \u00b6 processed features \\(\\rightarrow\\) final predictions for each point Note: Both input and output blocks use a sequence of multiple \\(1 \\times 1\\) convolutional layers to produce independent feature representations for each point.","title":"RSNet (Recurrent Slice Networks)"},{"location":"articles/RSNet/main/#problem-with-other-semantic-segmentation-networks","text":"Not modelling required dependencies between cloud points.","title":"Problem with other semantic segmentation networks"},{"location":"articles/RSNet/main/#key-component-in-rsnet","text":"","title":"Key component in RSNet"},{"location":"articles/RSNet/main/#lightweight-local-dependency-module","text":"The local dependency module is highly efficient and has the time complexity of the slice pooling/unpooling layer as \\(O(n)\\) w.r.t the number of input points and \\(O(1)\\) w.r.t the local context resolutions.","title":"lightweight local dependency module"},{"location":"articles/RSNet/main/#rsnet-components","text":"","title":"RSNet Components"},{"location":"articles/RSNet/main/#input-feature-extraction-block","text":"input points \\(\\rightarrow\\) features","title":"Input feature extraction block"},{"location":"articles/RSNet/main/#local-dependency-module","text":"Combination of Slice pooling layer Bidirectional Recurrent Neural Network (RNN) layers Slice unpooling layer local context problem is solved by first projecting unordered points into ordered features and then applying traditional end-to-end learning algorithms The projection is achieved by a novel slice pooling layer Slice pooling layer input: features of unordered points \\(\\rightarrow\\) output: ordered sequence of aggregated features RNN layers Next, RNNs are applied to model dependencies in this sequence. Slice unpooling layer Finally, a slice unpooling layer assigns features in the sequence back to points .","title":"Local dependency module"},{"location":"articles/RSNet/main/#output-feature-extraction-block","text":"processed features \\(\\rightarrow\\) final predictions for each point Note: Both input and output blocks use a sequence of multiple \\(1 \\times 1\\) convolutional layers to produce independent feature representations for each point.","title":"Output feature extraction block"},{"location":"articles/RandLANet/main/","text":"Limitations of existing methods: \u00b6 Most approaches are limited to extremely 3D point clouds. methods like: PointNet PointNet++ PointCNN PCCN ShellNet Few methods can directly process large-scale point clouds, but they either rely on time-consuming preprocessing or computationally expensive voxelization steps . methods like: SPG FCPN TagentConv PCT Goal \u00b6 Develope a method that is Process large-scale point clouds directly Without block partition and block merging Take the whole geometry into consideration Computationally efficient & Memory efficient Without time-consuming preprocessing or voxelization steps inference a large-scale point cloud in a single pass Effective & Accurate Complex geometrix structures Capture and preserve the prominent features Key Problems \u00b6 Efficient point sampling to reduce memomry footprint and computational cost Effective local feature aggregatiion to capture the geometrical patterns So we have the problem of \"discarding useful features\" in Random Point Sampling. To solve this problem, Local Feature Aggregation is proposed. Local Feature Aggregation \u00b6 Local Spatial Encoding \u00b6 Find the neighboring points for each point by using KNN Explicitly encode the relative point position of all neighboring points so that the corresponding point features are always aware of their relative spatial locations. this tends to aid the network to captures the geometric patterns. Encoded relative point position features are concatenated with its corresponding point features. Attentive Pooling \u00b6 Goal: Aggregate the neighboring feature set Instead of using max or mean pooling to hard integrade the neighboring features (majority of information has been lost), we turn to the powerful attention mechanism to automatically learn important local features from the neighboring feature set. Computing attention scores : with a shared function \\(g\\) to learn a unique attention score for each feature Weighted Summation : \\(\\widetilde{f_i}\\) which is the aggregated features Dilated Redidual Block \u00b6 Since the large point clouds are going to be substantially down-sampled, it is desirable to significantly increase the receptive field of each point (so that geometric details are more likely to reserved even if some points are dropped). So multiple spatial encoding and attentive pooling units are stacked together with a skip connection as a dilated residual block. RandLA-Net Architechture \u00b6","title":"RandLA-Net"},{"location":"articles/RandLANet/main/#limitations-of-existing-methods","text":"Most approaches are limited to extremely 3D point clouds. methods like: PointNet PointNet++ PointCNN PCCN ShellNet Few methods can directly process large-scale point clouds, but they either rely on time-consuming preprocessing or computationally expensive voxelization steps . methods like: SPG FCPN TagentConv PCT","title":"Limitations of existing methods:"},{"location":"articles/RandLANet/main/#goal","text":"Develope a method that is Process large-scale point clouds directly Without block partition and block merging Take the whole geometry into consideration Computationally efficient & Memory efficient Without time-consuming preprocessing or voxelization steps inference a large-scale point cloud in a single pass Effective & Accurate Complex geometrix structures Capture and preserve the prominent features","title":"Goal"},{"location":"articles/RandLANet/main/#key-problems","text":"Efficient point sampling to reduce memomry footprint and computational cost Effective local feature aggregatiion to capture the geometrical patterns So we have the problem of \"discarding useful features\" in Random Point Sampling. To solve this problem, Local Feature Aggregation is proposed.","title":"Key Problems"},{"location":"articles/RandLANet/main/#local-feature-aggregation","text":"","title":"Local Feature Aggregation"},{"location":"articles/RandLANet/main/#local-spatial-encoding","text":"Find the neighboring points for each point by using KNN Explicitly encode the relative point position of all neighboring points so that the corresponding point features are always aware of their relative spatial locations. this tends to aid the network to captures the geometric patterns. Encoded relative point position features are concatenated with its corresponding point features.","title":"Local Spatial Encoding"},{"location":"articles/RandLANet/main/#attentive-pooling","text":"Goal: Aggregate the neighboring feature set Instead of using max or mean pooling to hard integrade the neighboring features (majority of information has been lost), we turn to the powerful attention mechanism to automatically learn important local features from the neighboring feature set. Computing attention scores : with a shared function \\(g\\) to learn a unique attention score for each feature Weighted Summation : \\(\\widetilde{f_i}\\) which is the aggregated features","title":"Attentive Pooling"},{"location":"articles/RandLANet/main/#dilated-redidual-block","text":"Since the large point clouds are going to be substantially down-sampled, it is desirable to significantly increase the receptive field of each point (so that geometric details are more likely to reserved even if some points are dropped). So multiple spatial encoding and attentive pooling units are stacked together with a skip connection as a dilated residual block.","title":"Dilated Redidual Block"},{"location":"articles/RandLANet/main/#randla-net-architechture","text":"","title":"RandLA-Net Architechture"},{"location":"articles/pointNet/pointNet/","text":"Previous works \u00b6 Point cloud is converted to other representations before it\u2019s fed to a deep neural network. PointNet \u00b6 Tasks \u00b6 Object Classification Object Part Segmentation Semantic Scene Parsing ... Challenges \u00b6 Unordered point set as input (invariant to permutations) Invariance under geometric transformations Permutation Invariance: Symmetric Function \u00b6 \\(f(x_1, x_2, \\cdots, x_n) \\equiv f(x_{\\pi_1}, x_{\\pi_2}, \\cdots, x_{\\pi_n}), \\qquad x_i \\in \\mathbb{R}^D\\) Examples: \\(f(x_1, x_2, \\cdots, x_n) = max\\{x_1, x_2, \\cdots, x_n\\}\\) \\(f(x_1, x_2, \\cdots, x_n) = x_1 + x_2 + \\cdots + x_n\\) Observe \\(f(x_1, x_2, \\cdots, x_n) = \\gamma \\circ g(h(x_1), \\cdots, h(x_n))\\) is symmetric if \\(g\\) is symmetric with this in mind, vanilla PointNet is constructed as below: which consists of: multi-layer perceptron (MLP) max pooling Geometric Transformations Invariance: Input Alignment by Transformer Network \u00b6 A regularizer is also used as shown below \\(L_{reg} = ||I - AA^T||_F^2\\) PointNet Classification Network \u00b6 PointNet Segmentation Network \u00b6 Resources \u00b6 3D Point Cloud Semantic Segmentation Using Deep Learning Techniques Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 652-660)","title":"PointNet"},{"location":"articles/pointNet/pointNet/#previous-works","text":"Point cloud is converted to other representations before it\u2019s fed to a deep neural network.","title":"Previous works"},{"location":"articles/pointNet/pointNet/#pointnet","text":"","title":"PointNet"},{"location":"articles/pointNet/pointNet/#tasks","text":"Object Classification Object Part Segmentation Semantic Scene Parsing ...","title":"Tasks"},{"location":"articles/pointNet/pointNet/#challenges","text":"Unordered point set as input (invariant to permutations) Invariance under geometric transformations","title":"Challenges"},{"location":"articles/pointNet/pointNet/#permutation-invariance-symmetric-function","text":"\\(f(x_1, x_2, \\cdots, x_n) \\equiv f(x_{\\pi_1}, x_{\\pi_2}, \\cdots, x_{\\pi_n}), \\qquad x_i \\in \\mathbb{R}^D\\) Examples: \\(f(x_1, x_2, \\cdots, x_n) = max\\{x_1, x_2, \\cdots, x_n\\}\\) \\(f(x_1, x_2, \\cdots, x_n) = x_1 + x_2 + \\cdots + x_n\\) Observe \\(f(x_1, x_2, \\cdots, x_n) = \\gamma \\circ g(h(x_1), \\cdots, h(x_n))\\) is symmetric if \\(g\\) is symmetric with this in mind, vanilla PointNet is constructed as below: which consists of: multi-layer perceptron (MLP) max pooling","title":"Permutation Invariance: Symmetric Function"},{"location":"articles/pointNet/pointNet/#geometric-transformations-invariance-input-alignment-by-transformer-network","text":"A regularizer is also used as shown below \\(L_{reg} = ||I - AA^T||_F^2\\)","title":"Geometric Transformations Invariance: Input Alignment by Transformer Network"},{"location":"articles/pointNet/pointNet/#pointnet-classification-network","text":"","title":"PointNet Classification Network"},{"location":"articles/pointNet/pointNet/#pointnet-segmentation-network","text":"","title":"PointNet Segmentation Network"},{"location":"articles/pointNet/pointNet/#resources","text":"3D Point Cloud Semantic Segmentation Using Deep Learning Techniques Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 652-660)","title":"Resources"},{"location":"articles/pointNet%2B%2B/main/","text":"Problem with PointNet \u00b6 PointNet does not capture local structures induced by the metric space in which points are present, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. PointNet++ \u00b6 A hierarchical neural network that applies PointNet recursively. How it works \u00b6 PointNet++ divides a set of points into overlapping local regions based on the underlying space's distance measure. Similar to CNNs, it extracts local features from tiny neighborhoods, capturing fine geometric structures, and then groups these local features into bigger units and processes them to produce higher-level features. This technique is repeated until the entire point set's features are obtained. It Solves two problems: How to generate the partitioning of the point set How to abstract sets of points or local features through a local feature learner PointNet uses only a single max-pooling operation to aggregate the whole point set, while the PointNet++ builds a hierarchical grouping of points and progressively abstract larger local regions along the hierarchy. The hierarchical structure of PointNet++ is composed of abstraction levels and at each one, a set of points is processed and abstracted to produce a new set with fewer elements. The abstraction layer is made of three layers: Sampling layer: Selects a set of points from input points, which defines the centroids of local regions. Grouping layer: Constructs local region sets by finding \u201cneighboring\u201d points around the centroids. PointNet layer: Uses a mini-PointNet to encode local region patterns into feature vectors. Resources \u00b6 3D Point Cloud Semantic Segmentation Using Deep Learning Techniques Main Paper: Qi, C. R., Yi, L., Su, H., & Guibas, L. J. (2017). Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv preprint arXiv:1706.02413.","title":"PointNet++"},{"location":"articles/pointNet%2B%2B/main/#problem-with-pointnet","text":"PointNet does not capture local structures induced by the metric space in which points are present, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes.","title":"Problem with PointNet"},{"location":"articles/pointNet%2B%2B/main/#pointnet","text":"A hierarchical neural network that applies PointNet recursively.","title":"PointNet++"},{"location":"articles/pointNet%2B%2B/main/#how-it-works","text":"PointNet++ divides a set of points into overlapping local regions based on the underlying space's distance measure. Similar to CNNs, it extracts local features from tiny neighborhoods, capturing fine geometric structures, and then groups these local features into bigger units and processes them to produce higher-level features. This technique is repeated until the entire point set's features are obtained. It Solves two problems: How to generate the partitioning of the point set How to abstract sets of points or local features through a local feature learner PointNet uses only a single max-pooling operation to aggregate the whole point set, while the PointNet++ builds a hierarchical grouping of points and progressively abstract larger local regions along the hierarchy. The hierarchical structure of PointNet++ is composed of abstraction levels and at each one, a set of points is processed and abstracted to produce a new set with fewer elements. The abstraction layer is made of three layers: Sampling layer: Selects a set of points from input points, which defines the centroids of local regions. Grouping layer: Constructs local region sets by finding \u201cneighboring\u201d points around the centroids. PointNet layer: Uses a mini-PointNet to encode local region patterns into feature vectors.","title":"How it works"},{"location":"articles/pointNet%2B%2B/main/#resources","text":"3D Point Cloud Semantic Segmentation Using Deep Learning Techniques Main Paper: Qi, C. R., Yi, L., Su, H., & Guibas, L. J. (2017). Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv preprint arXiv:1706.02413.","title":"Resources"},{"location":"codes/DCGAN/main/","text":"GAN \u00b6 GANs are a framework for teaching a DL model to capture the training data\u2019s distribution so we can generate new data from that same distribution. GANs were invented by Ian Goodfellow in 2014. They are made of two distinct models, a generator and a discriminator Job of generator: generating 'fake' images that look like the training images. Job of discriminator: look at an image and output whether or not it is a real training image or a fake image from the generator During training, the generator is constantly trying to outsmart the discriminator by generating better and better fakes, while the discriminator is working to become a better detective and correctly classify the real and fake images. The equilibrium of this game is when the generator is generating perfect fakes that look as if they came directly from the training data, and the discriminator is left to always guess at 50% confidence that the generator output is real or fake. Some notations \u00b6 \\(D(x)\\) is the discriminator network which outputs the (scalar) probability that \\(x\\) came from training data rather than the generator Here, since we are dealing with images, \\(x\\) is an image of size \\(3 \\times 64 \\times 64\\) \\(D(x)\\) should be high when \\(x\\) comes from training data and low when \\(x\\) comes from the generator \\(G(z)\\) represents the generator function which maps the latent vector \\(z\\) to data-space. \\(z\\) is a latent space vector sampled from a standard normal distribution The goal of \\(G\\) is to estimate the distribution that the training data comes from ( \\(p_{data}\\) ) so it can generate fake samples from that estimated distribution ( \\(p_g\\) ) So, \\(D(G(z))\\) is the probability (scalar) that the output of the generator \\(G\\) is a real image. Loss function \u00b6 \\(D\\) and \\(G\\) play a minimax game in which \\(D\\) tries to maximize the probability it correctly classifies reals and fakes \\(\\rightarrow log(D(x))\\) \\(G\\) tries to minimize the probability that \\(D\\) will predict its outputs are fake \\(\\rightarrow log(1-D(G(z)))\\) From the paper, the GAN loss function is: \\(min_G \\; max_D \\; V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[log(D(x))] + \\mathbb{E}_{z \\sim p_z(z)}[log(1-D(G(z)))]\\) In theory, the solution to this minimax game is where \\(p_g = p_{data}\\) , and the discriminator guesses randomly if the inputs are real or fake. However, the convergence theory of GANs is still being actively researched and in reality models do not always train to this point. DCGAN \u00b6 Deep Convolutional Generative Adversarial Networks (DCGAN) is a direct extension of the GAN, except that it explicitly uses convolutional and convolutional-transpose layers in the discriminator and generator, respectively. More details of implementation can be found in the link below Source Code \u00b6 Link Resources \u00b6 DCGAN Tutorial - PyTorch","title":"DCGAN"},{"location":"codes/DCGAN/main/#gan","text":"GANs are a framework for teaching a DL model to capture the training data\u2019s distribution so we can generate new data from that same distribution. GANs were invented by Ian Goodfellow in 2014. They are made of two distinct models, a generator and a discriminator Job of generator: generating 'fake' images that look like the training images. Job of discriminator: look at an image and output whether or not it is a real training image or a fake image from the generator During training, the generator is constantly trying to outsmart the discriminator by generating better and better fakes, while the discriminator is working to become a better detective and correctly classify the real and fake images. The equilibrium of this game is when the generator is generating perfect fakes that look as if they came directly from the training data, and the discriminator is left to always guess at 50% confidence that the generator output is real or fake.","title":"GAN"},{"location":"codes/DCGAN/main/#some-notations","text":"\\(D(x)\\) is the discriminator network which outputs the (scalar) probability that \\(x\\) came from training data rather than the generator Here, since we are dealing with images, \\(x\\) is an image of size \\(3 \\times 64 \\times 64\\) \\(D(x)\\) should be high when \\(x\\) comes from training data and low when \\(x\\) comes from the generator \\(G(z)\\) represents the generator function which maps the latent vector \\(z\\) to data-space. \\(z\\) is a latent space vector sampled from a standard normal distribution The goal of \\(G\\) is to estimate the distribution that the training data comes from ( \\(p_{data}\\) ) so it can generate fake samples from that estimated distribution ( \\(p_g\\) ) So, \\(D(G(z))\\) is the probability (scalar) that the output of the generator \\(G\\) is a real image.","title":"Some notations"},{"location":"codes/DCGAN/main/#loss-function","text":"\\(D\\) and \\(G\\) play a minimax game in which \\(D\\) tries to maximize the probability it correctly classifies reals and fakes \\(\\rightarrow log(D(x))\\) \\(G\\) tries to minimize the probability that \\(D\\) will predict its outputs are fake \\(\\rightarrow log(1-D(G(z)))\\) From the paper, the GAN loss function is: \\(min_G \\; max_D \\; V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[log(D(x))] + \\mathbb{E}_{z \\sim p_z(z)}[log(1-D(G(z)))]\\) In theory, the solution to this minimax game is where \\(p_g = p_{data}\\) , and the discriminator guesses randomly if the inputs are real or fake. However, the convergence theory of GANs is still being actively researched and in reality models do not always train to this point.","title":"Loss function"},{"location":"codes/DCGAN/main/#dcgan","text":"Deep Convolutional Generative Adversarial Networks (DCGAN) is a direct extension of the GAN, except that it explicitly uses convolutional and convolutional-transpose layers in the discriminator and generator, respectively. More details of implementation can be found in the link below","title":"DCGAN"},{"location":"codes/DCGAN/main/#source-code","text":"Link","title":"Source Code"},{"location":"codes/DCGAN/main/#resources","text":"DCGAN Tutorial - PyTorch","title":"Resources"},{"location":"daily_report/main/","text":"7/5/21 - 7/9/21 and 7/12/21 - 7/16/21 \"Read about some methods for point cloud Segmentation: PointNet PointNet++ RandLA-Net RSNet (Recurrent Slice Networks) Some other methods Videos of some of the above approaches (for better understanding) Read about: GitPod Github Actions GitPod Customized Docker container Docker Docker Container Action Read a bit about: NVIDIA GPU Cloud NVIDIA Docker Explore NGC Container library Find suitable containers that can be used for ML and AI of point cloud data (Couldn't find anything that looked promising) Read about the basics: Lidar/Structure from motion data types ... ... 7/19/21 GAN Paper Video (for more details) 7/20/21 DCGAN (Implementation using PyTorch) In person meeting 7/21/21 DCGAN (Implementation) Documentation in Github Pages 7/22/21 Ubuntu Setup Pytorch Notebook Dockerfile 7/23/21 Searching through different problems that we can work on CS231n (CV Course) 7/26/21 Visualization of point clouds with CloudCompare Container Camp CS231n (Computer Vision Course) 7/27/21 PointNet Container Camp 7/28/21 Docker Container Camp 7/29/21 PointNet Implementation (Using PyTorch) 7/30/21 PointNet Classification Implementation Meeting with Travis on PhytoOracle Project Deep Learning Course 8/2/21 Playing with gpu06 (SSH + Docker + ...) Testing My PyTorch Docker Image Testing docker in gpu06 8/3/21 Python Scripts of PointNet Classficiation in gpu06 + github Container Camp Deep Learning Course 8/4/21 PointNet Classification + Documentation + Plots + ... Deep Learning Course 8/5/21 PointNet Segmentation 8/9/21 PointNet Segmentation Documentation Find Good labeling tool 8/10/21 PointNet++ Paper Deep Learning Course 8/11/21 Finding labeling tool Deep Learning Course DL Notebooks 8/12/2021 Deep Learning Course Data preparation scripts 8/13/2021 Deep Learning Course Pipeline of annotating data Set up new laptop 8/14/2021 Setup ubuntu & ... Workning with supervisely 8/16/21 splitting 3D Lettuce dataset to batches for labeling Deep Learnin Course 8/17/21 Annotating data with supervisely Scripts for conversion in gihub Deep Learnin Course 8/18/21 Rand-LA Net Paper (half) Deep Learning Course Annotate a batch of data Create a tool for visualization of labeled pointclouds 8/19/21 Add more options to visualization tool Rand-LA Net Paper (half) Deep Learning Course 8/20/21 Rand-LA Net PyTorch Implementation Deep Learning Course 8/23/21 Rand-LA Net PyTorch Implementation + Results + Github Annotating PhytoOracle Dataset 8/24/21 PointNet++ Imeplementation + Results + Github Annotating PhytoOracle Dataset 8/25/21 Annotating PhytoOracle Dataset 3D Lettuce Soil Segmentation (PointNet) + Result 8/26/21 PointNet++ and RandLA-Net on Lettuce Soil Segmentation + Results Annotating PhytoOracle Dataset 8/27/21 Exploring other methods Annotating PhytoOracle Dataset 8/28/21 Annotating PhytoOracle Dataset 8/30/21 Autoencoder + pytorch implementation playing with annotator Computer Vision Course 8/31/21 Variational Autoencoder + pytorch implementation Annotating with Supervisely Computer Vision Course 9/1/21 Read a bit about ConvTranspose Annotating with Supervisely Computer Vision Course 9/2/21 Preparing for slides and etc. for presentation 9/7/21 Computer Vision Course Docker tut + test 9/8/21 Computer Vision Course Annotation 9/9/21 KNN for upsampling + multiprocessing 9/10/21 Dockerization of the project + Scripts + ... 9/13/21 Retrain models with new data Test new models in dockerized env. Image on dockerhub Computer Vision Course 9/14/21 GUI for Lettuce/Soil PointClouds using tkinter Dockerization of the GUI Test on different batch Computer Vision Course 9/15/21 DGCNN Paper 9/18/21 DGCNN Paper DGCNN Implementation on airplanes dataset 9/20/21 DGCNN re-Implementation on lettuce dataset Improve DGCNN by modifying it and make it simpler (prevent overfitting) 9/21/21 Re-Evaluation of all models on a fixed split of dataset Tunning the training of DGCNN 9/22/21 Evaluation of PointNet++, RandLANet and DGCNN on a batch of data Some labeling Computer Vision course 9/23/21 Dockerize training of the PhytoOracle dataset Trying to use CUDA (for libraries) in built stage of docker (no completed) 9/24/21 Containerization of train + predict - ... 9/25/21 Computer Vision Course Computer Vision HW (HW1 Q1 Prac.) 9/26/21 Computer Vision Course Test Containerized Environment Test Predic on a different batch (Next : Start writing paper)","title":"Daily Report"},{"location":"tasks/task_1/","text":"Data Discovery Explore PhytoOracle Data archive and identify test data sets to use Convert orthomosaic and 2D elevation models (Digital Surface Models DSM, and Digital Terrain Models (DTM)) to Cloud Optimized Geotiff https://www.cogeo.org/ Test Viewer in Browser with data from CyVerse Convert LAS and LAZ to Entwine Point Tiles https://entwine.io/ Test Viewer in Browser with data from in CyVerse Develop Docker Containers Explore NVIDIA GPU Cloud and identify containers (PyTorch, Tensorflow, etc) Explore NVIDIA pre-trained models and identify potential uses Create GitHub Repository with custom CyVerse Dockerfiles for deployment in VICE Create Tools and Apps for running containers in CyVerse Test Publish","title":"First Task"},{"location":"tasks/task_2/","text":"Create a logbook for daily updates Generate a more manuscript-like framework in these /docs , e.g. Introduction, Methods, Results, Discussion, Conclusions Begin working with PhytoOracle datasets running PointNet Segmentation training.","title":"Second Task"}]}